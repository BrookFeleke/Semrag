

# Improving RAG with Semantic Chunking, Query Expansion, Re-Ranking, and Prompt Engineering 

## Introduction

This project is a RAG application that helps ask questions to the Competent Program Evolution thesis paper. It attempts to enhance the Retrieval-Augmented Generation (RAG) pipeline by integrating semantic chunking  query expansion, re-ranking, and prompt engineering to improve the accuracy and relevance of retrieved documents and the quality of generated responses. RAG pipelines combine information retrieval and natural language generation to answer queries based on retrieved data. This project illustrates how these techniques improve each stage of the process.

## Key Concepts
### Chunking Strategy and Vector Store Improvements
#### Unstructured PDF Loader: Topic-Based Splitting
The project uses the **Unstructured PDF Loader** to split documents based on **topics** rather than arbitrarily dividing content. This is achieved through the `partition_pdf` function with the `chunking_strategy` set to `"by_title"`, ensuring the document is split into coherent sections that reflect the document's logical structure.
```python
def parse_pdf(filePath: str):
    raw_pdf_elements = partition_pdf(
        filename=filePath,
        chunking_strategy="by_title",  # Splitting based on document titles
        max_characters=4000,
        new_after_n_chars=3800,
        combine_text_under_n_chars=2000
    )
    return raw_pdf_elements
```

By splitting based on **titles** and sections, the documents are chunked in a way that preserves the topic flow, making the resulting chunks more meaningful for retrieval and embedding. This strategy enhances the relevance of the retrieved chunks during queries.
#### Semntic Chunking
The project utilizes **chunking** to split large documents into smaller, coherent pieces for better retrieval and embedding. This is done using two methods:
- **SemanticChunker**: Splits text based on semantic coherence, ensuring each chunk maintains logical meaning.
These strategies improve the **vector store** by ensuring that each chunk is more relevant, retrievable, and efficiently embedded, leading to better retrieval performance.

```python
text_splitter = SemanticChunker(embedding_function, breakpoint_threshold_type="percentile")
docs = [text_splitter.split_text(text) for text in docs]
```

Chunking ensures better search results by creating meaningful, smaller document sections for embedding and retrieval in the vector store.
### Query Expansion
The query expansion process in this project enhances the user query by appending additional context generated by a Large Language Model (LLM). This helps retrieve documents that are more aligned with the underlying semantics of the user's request.

```python
def expand_query(query):
    context = """Competent Program Evolution - ..."""
    model = genai.GenerativeModel(model_name='gemini-pro')
    messages = [
        {
            "role": "model",
            "parts": [
                "You are a highly intelligent prompt engineer designed to expand and optimize user queries...",
                f"Query: #^{query}^#",
                f"Context: #^{context}^#"
            ]
        }
    ]
    response = model.generate_content(messages)
    return response.candidates[0].content.parts[0].text.replace("#^", "").replace("^#", "")
```

The **expand_query** function uses the Gemini API to append relevant keywords and context to the original query. This enriched query improves document retrieval by increasing the precision of search results based on semantic relevance.

### Re-Ranking

After retrieving documents from the vector store, this project implements **re-ranking** based on cosine similarity between the query and the retrieved documents. This ensures the most contextually relevant documents are prioritized.

```python
def get_relevant_context_from_db(query):
    embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vector_db = Chroma(persist_directory="./comprog_chroma_db", embedding_function=embedding_model)

    initial_search_results = vector_db.similarity_search(query, k=20)
    query_embedding = np.array(embedding_model.embed_query(query)).reshape(1, -1)

    reranked_results = []
    for result in initial_search_results:
        result_embedding = np.array(embedding_model.embed_query(result.page_content)).reshape(1, -1)
        c_similarity = cosine_similarity(query_embedding, result_embedding)
        reranked_results.append((result, c_similarity[0][0]))

    reranked_results.sort(key=lambda x: x[1], reverse=True)
    context = "\n".join([result.page_content for result, _ in reranked_results[:10]])
    return context
```

In this process, documents are first retrieved using vector similarity search. The re-ranking stage then sorts these results based on their cosine similarity scores to the query, returning only the most relevant documents.

### Prompt Engineering

The **generate_answer** function demonstrates **prompt engineering** by carefully constructing a prompt to maximize the performance of the Gemini LLM. The structure of the prompt ensures that the model generates responses based only on the relevant context provided. Adding Domain specific instruction will greatly affect the output of the LLM.

```python
def generate_answer(query, context):
    model = genai.GenerativeModel(model_name='gemini-pro')
    messages = [
        {
            "role": "model",
            "parts": [
                "You are a highly intelligent research assistant tasked with answering specific research-related questions...",
                f"Query: #^^{query}^^#",
                f"Context: #^^{context}^^#"
            ]
        }
    ]
    answer = model.generate_content(messages)
    return answer.candidates[0].content.parts[0].text.replace("#^^", "").replace("^^#", "")
```

By engineering the prompt to include explicit instructions and only the necessary context, this function ensures that the responses are accurate and grounded in the relevant data, avoiding hallucinations or unrelated content.

## Setup Instructions

To set up this project on your local machine, follow these steps:

### 1. **Clone the Repository**

First, clone the repository to your local machine:

```bash
git clone https://github.com/BrookFeleke/Semrag.git
cd Semrag
```

### 2. **Install Dependencies**

You can install these packages using the following command:

```bash
pip install -r requirements.txt
```

### 3. **Configure Environment Variables**

The project requires an API key for Gemini AI, which needs to be stored in an `.env` file. Create a file named `.env` in the root directory of the project with the following content:

```dotenv
GEMINI_API_KEY=your_gemini_api_key_here
```

Replace `your_gemini_api_key_here` with your actual Gemini API key. This key is necessary for the project to interact with the Gemini AI API.

### 4. **Run the Application**

With everything set up, you can now run the Flask application using the following command:

```bash
python rag.py
```

The application will start, and you can access it by navigating to `http://127.0.0.1:5000/` in your web browser.


### Screenshots

![Website Screenshot](./images/imprag.png)
